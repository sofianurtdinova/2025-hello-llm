{
    "Babelscape/wikineural-multilingual-ner": {
        "Babelscape/wikineural": {
            "accuracy": 0.99111
        }
    },
    "EleutherAI/gpt-neo-125m": {
        "jtatman/databricks-dolly-8k-qa-open-close": {
            "bleu": 0.02232,
            "rouge": 0.10704
        },
        "lionelchg/dolly_open_qa": {
            "bleu": 0.01673,
            "rouge": 0.10149
        },
        "tatsu-lab/alpaca": {
            "bleu": 0.01784,
            "rouge": 0.10346
        },
        "truthful_qa": {
            "bleu": 0.00937,
            "rouge": 0.05331
        }
    },
    "EleutherAI/pythia-160m-deduped": {
        "jtatman/databricks-dolly-8k-qa-open-close": {
            "bleu": 0.02376,
            "rouge": 0.10681
        },
        "lionelchg/dolly_open_qa": {
            "bleu": 0.01307,
            "rouge": 0.08999
        },
        "tatsu-lab/alpaca": {
            "bleu": 0.01726,
            "rouge": 0.10478
        },
        "truthful_qa": {
            "bleu": 0.00685,
            "rouge": 0.04704
        }
    },
    "Helsinki-NLP/opus-mt-en-fr": {
        "enimai/MuST-C-fr": {
            "bleu": 0.45351
        }
    },
    "Helsinki-NLP/opus-mt-ru-en": {
        "shreevigneshs/iwslt-2023-en-ru-train-val-split-0.2": {
            "bleu": 0.25689
        }
    },
    "Helsinki-NLP/opus-mt-ru-es": {
        "nuvocare/Ted2020_en_es_fr_de_it_ca_pl_ru_nl": {
            "bleu": 0.23257
        }
    },
    "IlyaGusev/rubertconv_toxic_clf": {
        "Arsive/toxicity_classification_jigsaw": {
            "f1": 0.67000
        },
        "s-nlp/en_paradetox_toxicity": {
            "f1": 0.68999
        }
    },
    "JackFram/llama-68m": {
        "jtatman/databricks-dolly-8k-qa-open-close": {
            "bleu": 0.01741,
            "rouge": 0.08982
        },
        "lionelchg/dolly_open_qa": {
            "bleu": 0.01410,
            "rouge": 0.09541
        },
        "tatsu-lab/alpaca": {
            "bleu": 0.01191,
            "rouge": 0.08936
        },
        "truthful_qa": {
            "bleu": 0.00650,
            "rouge": 0.05480
        }
    },
    "MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli": {
        "mnli": {
            "accuracy": 0.90000
        }
    },
    "UrukHan/t5-russian-summarization": {
        "CarlBrendt/Summ_Dialog_News": {
            "bleu": 0.00482,
            "rouge": 0.08133
        },
        "IlyaGusev/gazeta": {
            "bleu": 0.00134,
            "rouge": 0.09582
        },
        "d0rj/curation-corpus-ru": {
            "bleu": 0.00000,
            "rouge": 0.12565
        }
    },
    "VMware/electra-small-mrqa": {
        "HuggingFaceH4/no_robots": {
            "squad": 7.99780
        },
        "lionelchg/dolly_closed_qa": {
            "squad": 19.88021
        },
        "starmpcc/Asclepius-Synthetic-Clinical-Notes": {
            "squad": 3.58850
        }
    },
    "XSY/albert-base-v2-imdb-calssification": {
        "imdb": {
            "f1": 0.73999
        }
    },
    "aiknowyou/it-emotion-analyzer": {
        "dair-ai/emotion": {
            "f1": 0.30999
        }
    },
    "blanchefort/rubert-base-cased-sentiment-rusentiment": {
        "blinoff/healthcare_facilities_reviews": {
            "f1": 0.89000
        },
        "blinoff/kinopoisk": {
            "f1": 0.56999
        }
    },
    "cointegrated/rubert-base-cased-nli-threeway": {
        "cointegrated/nli-rus-translated-v2021": {
            "accuracy": 0.78000
        },
        "xnli": {
            "accuracy": 0.30999
        }
    },
    "cointegrated/rubert-tiny-bilingual-nli": {
        "terra": {
            "accuracy": 0.42999
        }
    },
    "cointegrated/rubert-tiny-toxicity": {
        "OxAISH-AL-LLM/wiki_toxic": {
            "f1": 0.80000
        }
    },
    "cointegrated/rubert-tiny2-cedr-emotion-detection": {
        "seara/ru_go_emotions": {
            "f1": 0.11000
        }
    },
    "cross-encoder/qnli-distilroberta-base": {
        "qnli": {
            "accuracy": 0.91000
        }
    },
    "dmitry-vorobiev/rubert_ria_headlines": {
        "CarlBrendt/Summ_Dialog_News": {
            "bleu": 0.00126,
            "rouge": 0.04966
        },
        "IlyaGusev/gazeta": {
            "bleu": 0.00038,
            "rouge": 0.09219
        },
        "d0rj/curation-corpus-ru": {
            "bleu": 0.00000,
            "rouge": 0.09521
        },
        "trixdade/reviews_russian": {
            "bleu": 0.00001,
            "rouge": 0.00000
        }
    },
    "dslim/distilbert-NER": {
        "eriktks/conll2003": {
            "accuracy": 0.99333
        }
    },
    "fabriceyhc/bert-base-uncased-ag_news": {
        "ag_news": {
            "f1": 0.95999
        }
    },
    "google-t5/t5-small": {
        "RocioUrquijo/en_de": {
            "bleu": 0.24444
        }
    },
    "mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization": {
        "ccdv/govreport-summarization": {
            "bleu": 0.00000,
            "rouge": 0.07016
        },
        "ccdv/pubmed-summarization": {
            "bleu": 0.00607,
            "rouge": 0.12685
        },
        "cnn_dailymail": {
            "bleu": 0.05655,
            "rouge": 0.22158
        }
    },
    "mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization": {
        "ccdv/govreport-summarization": {
            "bleu": 0.00000,
            "rouge": 0.06920
        },
        "ccdv/pubmed-summarization": {
            "bleu": 0.00567,
            "rouge": 0.12992
        },
        "cnn_dailymail": {
            "bleu": 0.05981,
            "rouge": 0.22063
        }
    },
    "nandakishormpai/t5-small-machine-articles-tag-generation": {
        "ccdv/govreport-summarization": {
            "bleu": 0.00000,
            "rouge": 0.02879
        },
        "ccdv/pubmed-summarization": {
            "bleu": 0.00048,
            "rouge": 0.07849
        },
        "cnn_dailymail": {
            "bleu": 0.06624,
            "rouge": 0.17864
        }
    },
    "papluca/xlm-roberta-base-language-detection": {
        "papluca/language-identification": {
            "f1": 1.00000
        }
    },
    "s-nlp/russian_toxicity_classifier": {
        "d0rj/rudetoxifier_data": {
            "f1": 1.00000
        },
        "s-nlp/ru_non_detoxified": {
            "f1": 0.77000
        },
        "s-nlp/ru_paradetox_toxicity": {
            "f1": 0.72999
        }
    },
    "tatiana-merz/turkic-cyrillic-classifier": {
        "tatiana-merz/cyrillic_turkic_langs": {
            "f1": 0.98999
        }
    },
    "test_Helsinki-NLP/opus-mt-en-fr": {
        "enimai/MuST-C-fr": {
            "bleu": 0.44680
        }
    },
    "test_JackFram/llama-68m": {
        "tatsu-lab/alpaca": {
            "bleu": 0.00000,
            "rouge": 0.09024
        }
    },
    "test_VMware/electra-small-mrqa": {
        "starmpcc/Asclepius-Synthetic-Clinical-Notes": {
            "squad": 1.68774
        }
    },
    "test_aiknowyou/it-emotion-analyzer": {
        "dair-ai/emotion": {
            "f1": 0.29999
        }
    },
    "test_cointegrated/rubert-base-cased-nli-threeway": {
        "cointegrated/nli-rus-translated-v2021": {
            "accuracy": 0.80000
        }
    },
    "test_mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization": {
        "cnn_dailymail": {
            "bleu": 0.04108,
            "rouge": 0.23686
        }
    },
    "timpal0l/mdeberta-v3-base-squad2": {
        "HuggingFaceH4/no_robots": {
            "squad": 15.44216
        },
        "RussianNLP/wikiomnia": {
            "squad": 31.66666
        },
        "lionelchg/dolly_closed_qa": {
            "squad": 17.87963
        },
        "sberquad": {
            "squad": 43.75757
        },
        "starmpcc/Asclepius-Synthetic-Clinical-Notes": {
            "squad": 2.78039
        }
    }
}
